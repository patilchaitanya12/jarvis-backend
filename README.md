# ğŸ§  JARVIS Backend

AI Chatbot Backend built with FastAPI + TensorFlow + LLM fallback.

---

## ğŸš€ Tech Stack

- FastAPI
- TensorFlow (Intent Classification)
- NumPy
- HuggingFace API (LLM fallback)
- uv (Astral Python package manager)
- Docker
- CORS middleware

---

## ğŸ§  Architecture Flow

User Message
    â†“
Intent Classification Model (TensorFlow)
    â†“
If confidence > threshold â†’ Rule-based response
Else â†’ LLM Fallback (HuggingFace)
    â†“
Structured JSON Response

---

## ğŸ“‚ Project Structure

app/
 â”œâ”€â”€ main.py
 â”œâ”€â”€ brain.py
 â”œâ”€â”€ routers/
 â”‚    â””â”€â”€ chat.py
 â”œâ”€â”€ schemas/
 â”‚    â””â”€â”€ chat.py
 â”œâ”€â”€ service/
 â”‚    â””â”€â”€ brain_service.py
 â””â”€â”€ intents.json

models/
 â”œâ”€â”€ chat_model.h5
 â”œâ”€â”€ tokenizer.pkl
 â””â”€â”€ label_encoder.pkl

Dockerfile
pyproject.toml
uv.lock

---

## âš™ï¸ Environment Variables

Create `.env`:

HUGGINGFACE_API_KEY=your_key_here
CONFIDENCE_THRESHOLD=0.75

---

## ğŸƒ Run Locally

Using uv:

uv sync
uv run uvicorn app.main:app --reload

Backend runs on:

http://localhost:8000

Swagger Docs:

http://localhost:8000/docs

---

## ğŸ³ Run With Docker

docker build -t jarvis-backend .
docker run -p 8000:8000 jarvis-backend

---

## ğŸ“¡ API Endpoint

POST /chat/

Request:
{
  "message": "Who made you?"
}

Response:
{
  "response": "I was made by Chaitanya.",
  "confidence": 1.0,
  "source": "rule"
}

---

## ğŸ”¥ Key Features

- Custom trained NLP intent classifier
- Confidence-based response routing
- LLM fallback mechanism
- Modular architecture
- Dockerized
- Production-ready API design

---

## ğŸ¯ Future Improvements

- Redis caching
- CI/CD pipeline
- Logging & monitoring
- Model retraining pipeline
- Rate limiting

---

Built by Chaitanya Patil ğŸš€